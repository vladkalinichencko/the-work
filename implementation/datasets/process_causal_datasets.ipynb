{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f76402",
   "metadata": {},
   "source": [
    "# Process Causal Datasets: ATOMIC and Causal News Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeda1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import ast # For safely evaluating string literals like lists/dicts\n",
    "import re\n",
    "\n",
    "# Define base path relative to the notebook location\n",
    "base_path = os.path.dirname(os.path.abspath('__file__')) # Gets the directory of the notebook\n",
    "\n",
    "# Define input file paths\n",
    "atomic_file = os.path.join(base_path, 'Atomic Data', 'v4_atomic_all_agg.csv')\n",
    "cnc_st1_train_file = os.path.join(base_path, 'CausalNewsCorpus-master', 'data', 'V2', 'train_subtask1.csv')\n",
    "cnc_st1_dev_file = os.path.join(base_path, 'CausalNewsCorpus-master', 'data', 'V2', 'dev_subtask1.csv')\n",
    "cnc_st2_train_file = os.path.join(base_path, 'CausalNewsCorpus-master', 'data', 'V2', 'train_subtask2_grouped.csv')\n",
    "cnc_st2_dev_file = os.path.join(base_path, 'CausalNewsCorpus-master', 'data', 'V2', 'dev_subtask2_grouped.csv')\n",
    "\n",
    "# Define output file paths\n",
    "output_atomic_csv = os.path.join(base_path, 'atomic_causal_pairs.csv')\n",
    "output_cnc_csv = os.path.join(base_path, 'cnc_causal_pairs.csv')\n",
    "\n",
    "print(f\"Atomic input: {atomic_file}\")\n",
    "print(f\"CNC ST1 Train input: {cnc_st1_train_file}\")\n",
    "print(f\"CNC ST1 Dev input: {cnc_st1_dev_file}\")\n",
    "print(f\"CNC ST2 Train input: {cnc_st2_train_file}\")\n",
    "print(f\"CNC ST2 Dev input: {cnc_st2_dev_file}\")\n",
    "print(f\"Atomic output: {output_atomic_csv}\")\n",
    "print(f\"CNC output: {output_cnc_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e22ace3",
   "metadata": {},
   "source": [
    "## 1. Process ATOMIC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74219e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "names = [ 'Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Heidi', 'Ivan', 'Judy', 'Kevin', 'Laura', 'Mallory', 'Niaj', 'Olivia', 'Peggy', 'Quentin', 'Rita', 'Sybil', 'Trent', 'Uma', 'Victor', 'Wendy', 'Xavier', 'Yasmine', 'Zane', 'Aaron', 'Bianca', 'Carter', 'Delia', 'Ethan', 'Fiona', 'Gavin', 'Holly', 'Isabel', 'Jack', 'Kara', 'Liam', 'Mona', 'Nolan', 'Oscar', 'Paula', 'Quinn', 'Ralph', 'Sophie', 'Tina', 'Ursula', 'Vince', 'Will', 'Xena', 'Yuri', 'Zoey', 'Amber', 'Blake', 'Cleo', 'Derek', 'Elsa', 'Felix', 'Gina', 'Hank', 'Ivy', 'Jonas', 'Kelsey', 'Lars', 'Mira', 'Nate', 'Opal', 'Perry', 'Quincy', 'Rosa', 'Sam', 'Tara', 'Ulric', 'Vera', 'Wade', 'Ximena', 'Yosef', 'Zelda', 'Aiden', 'Brielle', 'Colin', 'Daisy', 'Emil', 'Freya', 'Gage', 'Hazel', 'Iris', 'Jasper', 'Kian', 'Lila', 'Myles', 'Nina', 'Omar', 'Pia', 'Rex', 'Sage', 'Toby', 'Una', 'Violet', 'Wyatt' ]\n",
    "\n",
    "fillers = [ 'computer', 'plan', 'idea', 'project', 'device', 'solution', 'resource', 'tool', 'book', 'car', 'phone', 'strategy', 'document', 'report', 'presentation', 'email', 'message', 'gift', 'letter', 'assignment', 'recipe', 'map', 'key', 'password', 'ticket', 'invitation', 'contract', 'agreement', 'proposal', 'schedule', 'appointment', 'meeting', 'task', 'goal', 'challenge', 'opportunity', 'problem', 'question', 'answer', 'result', 'experiment', 'test', 'survey', 'analysis', 'review', 'summary', 'outline', 'draft', 'plan', 'blueprint', 'formula', 'method', 'procedure', 'routine', 'habit', 'custom', 'tradition', 'rule', 'law', 'policy', 'principle', 'value', 'belief', 'dream', 'wish', 'hope', 'ambition', 'desire', 'intention', 'purpose', 'mission', 'vision', 'target', 'objective', 'aim', 'priority', 'focus', 'theme', 'topic', 'subject', 'concept', 'notion', 'thought', 'memory', 'experience', 'event', 'incident', 'accident', 'story', 'tale', 'joke', 'anecdote', 'remark', 'comment', 'statement', 'declaration', 'announcement', 'notice', 'warning', 'alert' ]\n",
    "\n",
    "# Load ATOMIC data\n",
    "try:\n",
    "    atomic_df = pd.read_csv(atomic_file)\n",
    "    print(f\"Loaded ATOMIC data: {atomic_df.shape[0]} rows\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: ATOMIC file not found at {atomic_file}\")\n",
    "    atomic_df = None\n",
    "\n",
    "if atomic_df is not None:\n",
    "    # Columns containing effect annotations\n",
    "    effect_columns = ['oEffect', 'xEffect']\n",
    "\n",
    "    # Function to safely parse JSON-like lists\n",
    "    def safe_json_loads(x):\n",
    "        if isinstance(x, list):\n",
    "            return x\n",
    "        if isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
    "            try:\n",
    "                return ast.literal_eval(x)\n",
    "            except:\n",
    "                try:\n",
    "                    return json.loads(x)\n",
    "                except:\n",
    "                    return []\n",
    "        return []\n",
    "\n",
    "    # Parse JSON lists in effect columns\n",
    "    for col in effect_columns:\n",
    "        if col in atomic_df.columns:\n",
    "            atomic_df[col] = atomic_df[col].apply(safe_json_loads)\n",
    "    # Parse prefix column if available\n",
    "    if 'prefix' in atomic_df.columns:\n",
    "        atomic_df['prefix'] = atomic_df['prefix'].apply(safe_json_loads)\n",
    "\n",
    "    # Create list to store processed data\n",
    "    atomic_processed = []\n",
    "\n",
    "    # Iterate through the dataframe\n",
    "    for index, row in atomic_df.iterrows():\n",
    "        raw_event = row['event']\n",
    "        # Shuffle names for each row\n",
    "        shuffled_names = names.copy()\n",
    "        random.shuffle(shuffled_names)\n",
    "        nameX = shuffled_names[0]\n",
    "        nameY = shuffled_names[1] if len(shuffled_names) > 1 else shuffled_names[0]\n",
    "        filler = random.choice(fillers)\n",
    "        event_filled = raw_event.replace('PersonX', nameX).replace('PersonY', nameY)\n",
    "        cause = event_filled.replace('___', filler)\n",
    "        # Replace names in the effects as well\n",
    "        for col in effect_columns:\n",
    "            if col in row and isinstance(row[col], list):\n",
    "                valid_effects = [e for e in row[col] if isinstance(e, str) and e.lower() != 'none' and e.strip()]\n",
    "                for effect in set(valid_effects):\n",
    "                    effect_filled = effect.replace('PersonX', nameX).replace('PersonY', nameY).replace('___', filler)\n",
    "                    atomic_processed.append({\n",
    "                        'cause': cause,\n",
    "                        'effect': effect_filled\n",
    "                    })\n",
    "\n",
    "    # Create final DataFrame with only cause and effect\n",
    "    atomic_final_df = pd.DataFrame(atomic_processed, columns=['cause', 'effect'])\n",
    "\n",
    "    # Save to CSV\n",
    "    atomic_final_df.to_csv(output_atomic_csv, index=False)\n",
    "    print(f\"Saved enriched ATOMIC data to {output_atomic_csv}: {atomic_final_df.shape[0]} pairs\")\n",
    "    display(atomic_final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e23cd0",
   "metadata": {},
   "source": [
    "## 2. Process Causal News Corpus (CNC) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4577ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CNC data\n",
    "try:\n",
    "  cnc_st1_train = pd.read_csv(cnc_st1_train_file, index_col=0)\n",
    "  cnc_st1_dev = pd.read_csv(cnc_st1_dev_file, index_col=0)\n",
    "  cnc_st2_train = pd.read_csv(cnc_st2_train_file, index_col=0)\n",
    "  cnc_st2_dev = pd.read_csv(cnc_st2_dev_file, index_col=0)\n",
    "\n",
    "  # Combine train/dev for both subtasks\n",
    "  cnc_st1_df = pd.concat([cnc_st1_train, cnc_st1_dev])\n",
    "  cnc_st2_df = pd.concat([cnc_st2_train, cnc_st2_dev])\n",
    "  print(f\"Loaded CNC ST1 data: {cnc_st1_df.shape[0]} rows\")\n",
    "  print(f\"Loaded CNC ST2 data: {cnc_st2_df.shape[0]} rows\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "  print(f\"Error loading CNC files: {e}\")\n",
    "  cnc_st1_df = None\n",
    "  cnc_st2_df = None\n",
    "\n",
    "if cnc_st1_df is not None and cnc_st2_df is not None:\n",
    "  cnc_processed = []\n",
    "\n",
    "  # Helper function to extract cause/effect from a string with <ARG0> and <ARG1> tags\n",
    "  def extract_cause_effect(span_str):\n",
    "    cause = None\n",
    "    effect = None\n",
    "    # Find first <ARG0>...</ARG0>\n",
    "    cause_match = re.search(r'<ARG0>(.*?)</ARG0>', span_str)\n",
    "    if cause_match:\n",
    "      cause = cause_match.group(1).strip()\n",
    "    # Find first <ARG1>...</ARG1>\n",
    "    effect_match = re.search(r'<ARG1>(.*?)</ARG1>', span_str)\n",
    "    if effect_match:\n",
    "      effect = effect_match.group(1).strip()\n",
    "    return cause, effect\n",
    "\n",
    "  # Process Subtask 2 (Causal sentences with spans)\n",
    "  for index, row in cnc_st2_df.iterrows():\n",
    "    sentence = row['text']\n",
    "    pairs_str = row['causal_text_w_pairs']\n",
    "    try:\n",
    "      # Safely evaluate the string representation of the list\n",
    "      pairs_list = ast.literal_eval(pairs_str)\n",
    "      if isinstance(pairs_list, list):\n",
    "        for span_str in pairs_list:\n",
    "          if isinstance(span_str, str):\n",
    "            cause, effect = extract_cause_effect(span_str)\n",
    "            if cause and effect:\n",
    "              cnc_processed.append({\n",
    "                'sentence': sentence,\n",
    "                'cause': cause,\n",
    "                'effect': effect,\n",
    "                'is_causal': 1\n",
    "              })\n",
    "    except (ValueError, SyntaxError, TypeError):\n",
    "      pass\n",
    "\n",
    "  # Process Subtask 1 (Add non-causal sentences as negative examples)\n",
    "  non_causal_st1 = cnc_st1_df[cnc_st1_df['label'] == 0]\n",
    "  for index, row in non_causal_st1.iterrows():\n",
    "    # Check if this sentence index was already processed from ST2 (unlikely but possible)\n",
    "    # This check assumes indices might overlap and have different causality interpretations\n",
    "    # A simpler approach might be to just add all label=0 from ST1.\n",
    "    is_already_added = any(p['sentence'] == row['text'] for p in cnc_processed)\n",
    "    if not is_already_added:\n",
    "      cnc_processed.append({\n",
    "        'sentence': row['text'],\n",
    "        'cause': None, # No cause span for non-causal\n",
    "        'effect': None, # No effect span for non-causal\n",
    "        'is_causal': 0\n",
    "      })\n",
    "\n",
    "  # Create final DataFrame\n",
    "  cnc_final_df = pd.DataFrame(cnc_processed)\n",
    "\n",
    "  # Remove potential duplicates based on sentence, cause, effect\n",
    "  cnc_final_df = cnc_final_df.drop_duplicates()\n",
    "\n",
    "  # Save to CSV\n",
    "  cnc_final_df.to_csv(output_cnc_csv, index=False)\n",
    "  print(f\"Saved processed CNC data to {output_cnc_csv}: {cnc_final_df.shape[0]} entries\")\n",
    "  display(cnc_final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- First 5 rows of {output_atomic_csv} ---\")\n",
    "try:\n",
    "  display(pd.read_csv(output_atomic_csv).head())\n",
    "except FileNotFoundError:\n",
    "  print(\"File not found.\")\n",
    "\n",
    "print(f\"--- First 5 rows of {output_cnc_csv} ---\")\n",
    "try:\n",
    "  display(pd.read_csv(output_cnc_csv).head())\n",
    "except FileNotFoundError:\n",
    "  print(\"File not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
