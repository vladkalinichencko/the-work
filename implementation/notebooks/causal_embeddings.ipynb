{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e80fc1",
   "metadata": {},
   "source": [
    "# Causal-Embeddings Training Notebook\n",
    "\n",
    "This notebook implements a pipeline for training causal embeddings on a dataset of (cause, effect) pairs using PyTorch and e-CARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ca7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import sklearn.metrics as M\n",
    "import wandb\n",
    "import torch.nn.functional as F # Often needed\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e46509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01 # L2 regularization\n",
    "DROPOUT_PROB = 0.2 # dropout probability\n",
    "# MARGIN = 0.5    # margin for contrastive loss (No longer needed for InfoNCE)\n",
    "TEMPERATURE = 0.05 # scale for InfoNCE loss\n",
    "NUM_EPOCHS = 20   # Increased epochs, early stopping will handle the actual number\n",
    "# STEP_SIZE = 3    # step for scheduler (No longer needed for CosineAnnealingLR)\n",
    "# GAMMA = 0.5     # multiplier for scheduler (No longer needed for CosineAnnealingLR)\n",
    "BATCH_SIZE = 128   # batch size\n",
    "EARLY_STOPPING_PATIENCE = 2 # Epochs to wait for improvement before stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f6f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag: use pre-trained GPT2 embeddings or initialize them from scratch\n",
    "use_pretrained_embeds = True # False — for random init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6851158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading e-CARE dataset via HuggingFace datasets\n",
    "print(\"Loading dataset 12ml/e-CARE...\")\n",
    "ec_dataset = load_dataset(\"12ml/e-CARE\", split=\"train\")\n",
    "print(f\"Example record: {ec_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2e013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing tokenizer and model\n",
    "model_name = 'gpt2'\n",
    "print(f\"Model being used: {model_name}\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "if use_pretrained_embeds:\n",
    "  base_model = GPT2Model.from_pretrained(model_name)\n",
    "else:\n",
    "  base_model = GPT2Model(GPT2Config())\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class CausalEmbeddingModel(torch.nn.Module):\n",
    "  def __init__(self, base_model, dropout_prob=DROPOUT_PROB):\n",
    "    super().__init__()\n",
    "    self.encoder = base_model\n",
    "    self.dropout = torch.nn.Dropout(dropout_prob)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    last_hidden = outputs.last_hidden_state\n",
    "    mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "    summed = torch.sum(last_hidden * mask, 1)\n",
    "    counts = torch.clamp(mask.sum(1), min=1e-9)\n",
    "    embeddings = summed / counts\n",
    "    embeddings = self.dropout(embeddings)\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings\n",
    "\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "model = CausalEmbeddingModel(base_model, dropout_prob=DROPOUT_PROB).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f19b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InfoNCE Loss Function\n",
    "class InfoNCELoss(torch.nn.Module):\n",
    "  def __init__(self, temperature=TEMPERATURE):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "  def forward(self, emb1, emb2, labels):\n",
    "    # emb1: [batch_size, embed_dim] (e.g., cause embeddings)\n",
    "    # emb2: [batch_size, embed_dim] (e.g., effect embeddings)\n",
    "    # labels: [batch_size] (1 for positive pair, 0 for negative)\n",
    "\n",
    "    # Normalize embeddings (already done in model, but good practice)\n",
    "    emb1 = F.normalize(emb1, p=2, dim=1)\n",
    "    emb2 = F.normalize(emb2, p=2, dim=1)\n",
    "\n",
    "    # Calculate cosine similarity matrix (batch_size x batch_size)\n",
    "    # sim_matrix[i, j] = similarity between emb1_i and emb2_j\n",
    "    sim_matrix = torch.matmul(emb1, emb2.T) / self.temperature\n",
    "\n",
    "    # Create labels for cross-entropy: diagonal elements are positives\n",
    "    # We only calculate loss for the actual positive pairs in the input `labels`\n",
    "    positive_mask = (labels == 1)\n",
    "    if not positive_mask.any(): # Handle batches with no positive pairs if they occur\n",
    "       return torch.tensor(0.0, device=emb1.device, requires_grad=True) # Or handle as needed\n",
    "\n",
    "    # Select rows corresponding to positive causes\n",
    "    pos_sim_matrix = sim_matrix[positive_mask]\n",
    "\n",
    "    # Create target labels for these rows: the diagonal element (true effect) should be the target\n",
    "    # The index of the positive sample for emb1_i is i\n",
    "    # Need to find the original indices of the positive samples\n",
    "    positive_indices = torch.where(positive_mask)[0]\n",
    "    # The target label for the i-th positive sample (original index k=positive_indices[i])\n",
    "    # should correspond to the column k in the full similarity matrix.\n",
    "    # However, CrossEntropyLoss expects class indices relative to the input tensor (pos_sim_matrix).\n",
    "    # We need to map the global positive index `k` to the row index `i` within the `pos_sim_matrix`.\n",
    "    # This mapping is simply the range(number_of_positives).\n",
    "    num_positives = pos_sim_matrix.size(0)\n",
    "    targets = torch.arange(num_positives, device=emb1.device) # Target is the index of the positive pair within the filtered matrix\n",
    "\n",
    "    # Adjust targets based on the actual structure if emb1[i] positive is emb2[i]\n",
    "    # In our case, the positive pair for emb1[k] is emb2[k].\n",
    "    # So, the target column index in sim_matrix[k, :] is k.\n",
    "    # When we filter sim_matrix with positive_mask, the columns remain the same.\n",
    "    # We need the column indices corresponding to the positive pairs.\n",
    "    targets = positive_indices # The target column index is the original index of the positive pair\n",
    "\n",
    "    # Filter the similarity matrix columns to only include the relevant comparisons for the positive samples\n",
    "    # This is complex. Let's simplify: Use the standard CE loss where the target is the index of the positive sample.\n",
    "    # For emb1_i (where label_i=1), the positive is emb2_i.\n",
    "    # The logits are sim(emb1_i, emb2_j) for all j. Target is i.\n",
    "\n",
    "    # Recalculate using the standard approach for simplicity:\n",
    "    # Treat each emb1 as an anchor, its corresponding emb2 as positive (if label=1)\n",
    "    # and all other emb2 in the batch as negatives.\n",
    "\n",
    "    # Cosine similarity between corresponding pairs (potential positives)\n",
    "    sim_pos = torch.diag(sim_matrix) # Similarity between emb1_i and emb2_i\n",
    "\n",
    "    # Create logits for CrossEntropyLoss\n",
    "    # For each emb1_i, the logits are [sim(emb1_i, emb2_0), sim(emb1_i, emb2_1), ..., sim(emb1_i, emb2_{N-1})]\n",
    "    logits = sim_matrix\n",
    "\n",
    "    # Create targets: for row i, the target class is i (representing emb2_i)\n",
    "    targets = torch.arange(emb1.size(0), device=emb1.device)\n",
    "\n",
    "    # Calculate loss only for the positive pairs\n",
    "    loss = self.criterion(logits[positive_mask], targets[positive_mask])\n",
    "\n",
    "    return loss\n",
    "\n",
    "criterion = InfoNCELoss(temperature=TEMPERATURE) # Use the new loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c93c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of triples (cause, effect, label) from ec_dataset with hard negatives\n",
    "pairs = []\n",
    "# Collecting all possible effects for hard negatives\n",
    "all_effects = [item['choice1'] for item in ec_dataset] + [item['choice2'] for item in ec_dataset]\n",
    "for item in ec_dataset:\n",
    "  cause = item['premise']\n",
    "  # effect_pos — correct effect from field choice1/choice2 according to label\n",
    "  effect_pos = item['choice1'] if item['label'] == 0 else item['choice2']\n",
    "  # effect_neg — incorrect choice\n",
    "  effect_neg = item['choice2'] if item['label'] == 0 else item['choice1']\n",
    "  # Positive pair\n",
    "  pairs.append((cause, effect_pos, 1))\n",
    "  # Negative pair (contrastive)\n",
    "  pairs.append((cause, effect_neg, 0))\n",
    "  # Hard negative: random effect, not equal to the correct one\n",
    "  hn = random.choice([eff for eff in all_effects if eff != effect_pos])\n",
    "  pairs.append((cause, hn, 0))\n",
    "\n",
    "print(f\"Total pairs: {len(pairs)}\")\n",
    "print('Example of positive pair:', pairs[0])\n",
    "print('Example of negative pair:', pairs[1])\n",
    "print('Number of positive pairs:', sum(1 for x in pairs if x[2] == 1))\n",
    "print('Number of negative pairs:', sum(1 for x in pairs if x[2] == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f89cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "  project=\"causal-embeddings-ecare\",\n",
    "  config={\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"dropout_prob\": DROPOUT_PROB,\n",
    "    \"temperature\": TEMPERATURE,\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"model_name\": model_name,\n",
    "    \"use_pretrained\": use_pretrained_embeds,\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \"loss\": \"InfoNCE\",\n",
    "    \"early_stopping_patience\": EARLY_STOPPING_PATIENCE\n",
    "  }\n",
    ")\n",
    "# Log code for reproducibility\n",
    "wandb.save('causal_embeddings.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cc8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/eval\n",
    "train_data, eval_data = train_test_split(pairs, test_size=0.2, random_state=42)\n",
    "\n",
    "class LabeledCausalDataset(Dataset):\n",
    "  def __init__(self, data, tokenizer, max_length=64):\n",
    "    self.data = data\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_length = max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    cause, effect, label = self.data[idx]\n",
    "    if self.tokenizer.pad_token is None:\n",
    "      self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    enc1 = self.tokenizer(cause, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "    enc2 = self.tokenizer(effect, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "    return {\n",
    "      'input_ids1': enc1['input_ids'].squeeze(),\n",
    "      'attention_mask1': enc1['attention_mask'].squeeze(),\n",
    "      'input_ids2': enc2['input_ids'].squeeze(),\n",
    "      'attention_mask2': enc2['attention_mask'].squeeze(),\n",
    "      'label': torch.tensor(label, dtype=torch.float),\n",
    "      'cause_text': cause,\n",
    "      'effect_text': effect\n",
    "    }\n",
    "\n",
    "train_dataset = LabeledCausalDataset(train_data, tokenizer)\n",
    "eval_dataset = LabeledCausalDataset(eval_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with contrastive loss function and metrics logging\n",
    "train_losses = [] # Store epoch average train losses\n",
    "eval_aucs = []  # Store epoch evaluation AUCs\n",
    "best_eval_auc = 0.0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  batch_losses = [] # Track losses within an epoch\n",
    "  \n",
    "  for batch_idx, batch in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    emb1 = model(batch['input_ids1'].to(device), batch['attention_mask1'].to(device))\n",
    "    emb2 = model(batch['input_ids2'].to(device), batch['attention_mask2'].to(device))\n",
    "    labels = batch['label'].to(device)\n",
    "    \n",
    "    # Ensure there are positive samples for InfoNCE calculation in the batch\n",
    "    if (labels == 1).sum() == 0:\n",
    "      print(f\"Skipping batch {batch_idx+1} due to no positive samples.\")\n",
    "      continue # Skip batch if no positive pairs exist\n",
    "\n",
    "    loss = criterion(emb1, emb2, labels)\n",
    "\n",
    "    # Check for NaN loss\n",
    "    if torch.isnan(loss):\n",
    "      print(f\"NaN loss detected at Epoch {epoch+1}, Batch {batch_idx+1}. Skipping batch.\")\n",
    "      # Potentially log problematic batch data here if needed\n",
    "      wandb.log({\"problem_batch_data\": wandb.Table(data=[batch['cause_text'], batch['effect_text'], batch['label'].tolist()])})\n",
    "      continue # Skip optimizer step and loss accumulation\n",
    "    \n",
    "    loss.backward()\n",
    "    # Gradient clipping (optional but often helpful)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    current_loss = loss.item()\n",
    "    total_loss += current_loss\n",
    "    batch_losses.append(current_loss)\n",
    "    \n",
    "    # Log batch loss and learning rate to W&B\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    wandb.log({\n",
    "      \"batch_loss\": current_loss,\n",
    "      \"learning_rate\": current_lr,\n",
    "      \"epoch\": epoch + (batch_idx + 1) / len(train_loader) # Log fractional epoch\n",
    "    })\n",
    "    \n",
    "    if (batch_idx + 1) % 50 == 0: # Print progress every 50 batches\n",
    "      print(f\"Epoch {epoch+1} Batch {batch_idx+1}/{len(train_loader)} - Loss {current_loss:.4f}, LR: {current_lr:.2e}\")\n",
    "\n",
    "  avg_loss = total_loss / len(batch_losses) if batch_losses else 0 # Avoid division by zero\n",
    "  train_losses.append(avg_loss) # Append average loss for the epoch\n",
    "\n",
    "  # Evaluation step\n",
    "  model.eval()\n",
    "  y_true, y_scores = [], []\n",
    "  with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "      emb1 = model(batch['input_ids1'].to(device), batch['attention_mask1'].to(device))\n",
    "      emb2 = model(batch['input_ids2'].to(device), batch['attention_mask2'].to(device))\n",
    "      sim = F.cosine_similarity(emb1, emb2, dim=1).cpu().numpy()\n",
    "      y_scores.extend(sim.tolist())\n",
    "      y_true.extend(batch['label'].numpy().tolist())\n",
    "  \n",
    "  # Ensure evaluation data is valid before calculating AUC\n",
    "  if len(np.unique(y_true)) > 1: # Check if there's more than one class present\n",
    "    auc = M.roc_auc_score(y_true, y_scores)\n",
    "  else:\n",
    "    print(f\"Warning: Only one class present in evaluation labels for Epoch {epoch+1}. AUC cannot be calculated.\")\n",
    "    auc = 0.0 # Or handle as appropriate, e.g., np.nan\n",
    "\n",
    "  eval_aucs.append(auc) # Append AUC for the epoch\n",
    "  \n",
    "  # Log epoch metrics to W&B\n",
    "  wandb.log({\n",
    "    \"epoch\": epoch + 1, # Log integer epoch for epoch-level metrics\n",
    "    \"train_loss_epoch\": avg_loss,\n",
    "    \"eval_auc_epoch\": auc\n",
    "  })\n",
    "  \n",
    "  print(f\"Epoch {epoch+1}, Avg Train Loss: {avg_loss:.4f}, Eval AUC: {auc:.4f}\")\n",
    "  \n",
    "  # Early Stopping Check\n",
    "  if auc > best_eval_auc:\n",
    "    best_eval_auc = auc\n",
    "    epochs_no_improve = 0\n",
    "    # Save the best model checkpoint (optional)\n",
    "    best_model_path = os.path.join(wandb.run.dir, 'best_model.pth') # Save in W&B run dir\n",
    "    torch.save(model.state_dict(), best_model_path)\n",
    "    print(f\"New best model saved with AUC: {best_eval_auc:.4f}\")\n",
    "    wandb.save(best_model_path) # Save best model to W&B\n",
    "  else:\n",
    "    epochs_no_improve += 1\n",
    "    print(f\"No improvement in Eval AUC for {epochs_no_improve} epoch(s).\")\n",
    "\n",
    "  if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "    print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "    break # Exit training loop\n",
    "\n",
    "  # Step the scheduler (CosineAnnealingLR steps per epoch)\n",
    "  scheduler.step()\n",
    "\n",
    "print(\"Training finished.\")\n",
    "# Log final best AUC\n",
    "wandb.summary[\"best_eval_auc\"] = best_eval_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84171705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on the eval set with cosine similarity\n",
    "model.eval()\n",
    "y_true, y_scores = [], []\n",
    "with torch.no_grad():\n",
    "  for batch in eval_loader:\n",
    "    emb1 = model(batch['input_ids1'].to(device), batch['attention_mask1'].to(device))\n",
    "    emb2 = model(batch['input_ids2'].to(device), batch['attention_mask2'].to(device))\n",
    "    sim = torch.nn.functional.cosine_similarity(emb1, emb2, dim=1).cpu().numpy()\n",
    "    y_scores.extend(sim.tolist())\n",
    "    y_true.extend(batch['label'].numpy().tolist())\n",
    "auc = M.roc_auc_score(y_true, y_scores)\n",
    "print(f\"Eval AUC after e-CARE training: {auc:.4f}\")\n",
    "wandb.summary[\"final_eval_auc_ecare\"] = auc # Log final e-CARE AUC to the first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5605f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained causal embeddings in ../models\n",
    "os.makedirs(os.path.join(os.getcwd(), '..', 'models'), exist_ok=True)\n",
    "embeds = model.encoder.wte.weight.data.cpu()\n",
    "torch.save(embeds, os.path.join(os.getcwd(), '..', 'models', 'causal_embeds.pth'))\n",
    "print(\"Causal embeddings saved to ../models/causal_embeds.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d528386",
   "metadata": {},
   "source": [
    "#### Phase 2: Fine-tuning on Atomic and CNC\n",
    "\n",
    "Now we will continue training the model using data from Atomic and CNC datasets. We will use the same model weights that were obtained after training on e-CARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized creation of atomic_pairs\n",
    "print(\"Loading and processing Atomic dataset...\")\n",
    "atomic_df = pd.read_csv('../datasets/atomic_causal_pairs.csv')\n",
    "atomic_df.dropna(subset=['cause', 'effect'], inplace=True)\n",
    "\n",
    "cause_to_effects = atomic_df.groupby('cause')['effect'].apply(set).to_dict()\n",
    "all_atomic_effects_set = set(atomic_df['effect'].unique())\n",
    "print(f\"Found unique causes in Atomic: {len(cause_to_effects)}\")\n",
    "print(f\"Found unique effects in Atomic: {len(all_atomic_effects_set)}\")\n",
    "\n",
    "# Efficient creation of positive and negative pairs\n",
    "atomic_pairs = []\n",
    "for cause, effects in cause_to_effects.items():\n",
    "    # Positive pairs\n",
    "    for effect_pos in effects:\n",
    "        atomic_pairs.append((cause, effect_pos, 1))\n",
    "    # Preparing candidates for negative examples\n",
    "    total_negs = len(effects) * 2\n",
    "    neg_cands = list(all_atomic_effects_set - effects)\n",
    "    if len(neg_cands) >= total_negs:\n",
    "        neg_samples = random.sample(neg_cands, total_negs)\n",
    "    else:\n",
    "        fallback = neg_cands if neg_cands else list(all_atomic_effects_set)\n",
    "        neg_samples = random.choices(fallback, k=total_negs)\n",
    "    # Assigning two negative effects for each positive pair\n",
    "    for idx in range(len(effects)):\n",
    "        for neg_effect in neg_samples[2*idx:2*idx+2]:\n",
    "            atomic_pairs.append((cause, neg_effect, 0))\n",
    "\n",
    "print(f\"Created pairs from Atomic: {len(atomic_pairs)}\")\n",
    "print('Example of positive pair (Atomic):', next((p for p in atomic_pairs if p[2] == 1), None))\n",
    "print('Example of negative pair (Atomic):', next((p for p in atomic_pairs if p[2] == 0), None))\n",
    "print('Number of positive pairs (Atomic):', sum(1 for x in atomic_pairs if x[2] == 1))\n",
    "print('Number of negative pairs (Atomic):', sum(1 for x in atomic_pairs if x[2] == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and processing the CNC dataset\n",
    "print(\"\\nLoading and processing CNC dataset...\")\n",
    "cnc_df = pd.read_csv('../datasets/cnc_causal_pairs.csv')\n",
    "cnc_df.dropna(subset=['cause', 'effect', 'is_causal'], inplace=True)\n",
    "cnc_df = cnc_df[cnc_df['is_causal'] == 1]\n",
    "\n",
    "cause_to_effects_cnc = cnc_df.groupby('cause')['effect'].apply(set).to_dict()\n",
    "all_cnc_effects_set = set(cnc_df['effect'].unique())\n",
    "print(f\"Found unique causes in CNC: {len(cause_to_effects_cnc)}\")\n",
    "print(f\"Found unique effects in CNC: {len(all_cnc_effects_set)}\")\n",
    "\n",
    "cnc_pairs = []\n",
    "for cause, effects in cause_to_effects_cnc.items():\n",
    "    # Positive pairs\n",
    "    for effect_pos in effects:\n",
    "        cnc_pairs.append((cause, effect_pos, 1))\n",
    "    # Preparing candidates for negative examples (2 for each positive)\n",
    "    total_negs = len(effects) * 2\n",
    "    neg_cands = list(all_cnc_effects_set - effects)\n",
    "    if len(neg_cands) >= total_negs:\n",
    "        neg_samples = random.sample(neg_cands, total_negs)\n",
    "    else:\n",
    "        fallback = neg_cands if neg_cands else list(all_cnc_effects_set)\n",
    "        neg_samples = random.choices(fallback, k=total_negs)\n",
    "    # Adding two negative pairs for each positive one\n",
    "    for idx in range(len(effects)):\n",
    "        for neg_effect in neg_samples[2*idx:2*idx+2]:\n",
    "            cnc_pairs.append((cause, neg_effect, 0))\n",
    "\n",
    "print(f\"Created pairs from CNC: {len(cnc_pairs)}\")\n",
    "print('Example of positive pair (CNC):', next((p for p in cnc_pairs if p[2] == 1), None))\n",
    "print('Example of negative pair (CNC):', next((p for p in cnc_pairs if p[2] == 0), None))\n",
    "print('Number of positive pairs (CNC):', sum(1 for x in cnc_pairs if x[2] == 1))\n",
    "print('Number of negative pairs (CNC):', sum(1 for x in cnc_pairs if x[2] == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20373918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining data and creating DataLoaders for Phase 2\n",
    "print(\"\\nCombining data and preparing DataLoaders for Phase 2...\")\n",
    "combined_pairs_phase2 = atomic_pairs + cnc_pairs\n",
    "random.shuffle(combined_pairs_phase2) # Shuffling the combined dataset\n",
    "\n",
    "print(f\"Total pairs for Phase 2: {len(combined_pairs_phase2)}\")\n",
    "print('Number of positive pairs (Phase 2):', sum(1 for x in combined_pairs_phase2 if x[2] == 1))\n",
    "print('Number of negative pairs (Phase 2):', sum(1 for x in combined_pairs_phase2 if x[2] == 0))\n",
    "\n",
    "# Splitting into train/eval for Phase 2\n",
    "train_data_phase2, eval_data_phase2 = train_test_split(combined_pairs_phase2, test_size=0.15, random_state=43) # Using different ratio and random_state\n",
    "\n",
    "print(f\"Training set size (Phase 2): {len(train_data_phase2)}\")\n",
    "print(f\"Validation set size (Phase 2): {len(eval_data_phase2)}\")\n",
    "\n",
    "# Creating Dataset and DataLoader\n",
    "# Using the same LabeledCausalDataset class and the same tokenizer\n",
    "train_dataset_phase2 = LabeledCausalDataset(train_data_phase2, tokenizer)\n",
    "eval_dataset_phase2 = LabeledCausalDataset(eval_data_phase2, tokenizer)\n",
    "\n",
    "train_loader_phase2 = DataLoader(train_dataset_phase2, batch_size=BATCH_SIZE, shuffle=True)\n",
    "eval_loader_phase2 = DataLoader(eval_dataset_phase2, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders for Phase 2 are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a68fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of a new W&B run for Phase 2\n",
    "# End the previous run if it's still active (just in case)\n",
    "if wandb.run is not None:\n",
    "    print(\"Finishing previous W&B run...\")\n",
    "    try:\n",
    "        wandb.finish()\n",
    "    except Exception as e:\n",
    "        print(f\"Error finishing previous W&B run: {e}. Continuing...\")\n",
    "\n",
    "print(\"\\nInitializing W&B for Phase 2 training (Atomic + CNC)...\")\n",
    "wandb.init(\n",
    "  project=\"causal-embeddings-atomic-cnc\", # New project or group name\n",
    "  config={\n",
    "    \"learning_rate\": LEARNING_RATE / 5, # Can reduce LR for fine-tuning # CHANGED: Lowered LR further\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"dropout_prob\": DROPOUT_PROB,\n",
    "    \"temperature\": TEMPERATURE,\n",
    "    \"epochs\": NUM_EPOCHS, # Using the same max number of epochs\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"model_name\": model_name,\n",
    "    \"use_pretrained\": use_pretrained_embeds, # Indicates initial initialization\n",
    "    \"training_phase\": 2, # Adding phase flag\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \"loss\": \"InfoNCE\",\n",
    "    \"early_stopping_patience\": EARLY_STOPPING_PATIENCE\n",
    "  }\n",
    ")\n",
    "\n",
    "# Log code for reproducibility\n",
    "wandb.save('causal_embeddings.ipynb')\n",
    "\n",
    "# Reinitializing optimizer and scheduler for Phase 2\n",
    "# Using current model parameters but with potentially modified LR\n",
    "optimizer_phase2 = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE / 5, weight_decay=WEIGHT_DECAY) # CHANGED: Lowered LR further\n",
    "scheduler_phase2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_phase2, T_max=NUM_EPOCHS, eta_min=1e-8) # Can set eta_min even lower\n",
    "\n",
    "# Reset variables for early stopping and metrics\n",
    "train_losses_phase2 = [] \n",
    "eval_aucs_phase2 = []  \n",
    "best_eval_auc_phase2 = 0.0 # Starting from zero for this phase\n",
    "epochs_no_improve_phase2 = 0\n",
    "\n",
    "# Using the same criterion\n",
    "# criterion = InfoNCELoss(temperature=TEMPERATURE) # Already defined earlier\n",
    "\n",
    "print(\"Optimizer, scheduler and variables for Phase 2 initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260826cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for Phase 2 (Atomic + CNC)\n",
    "print(\"\\nStarting training Phase 2 (Atomic + CNC)...\")\n",
    "# Make sure the model is in training mode\n",
    "model.to(device) # Move the model to the appropriate device just in case\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss_phase2 = 0\n",
    "    batch_losses_phase2 = [] \n",
    "  \n",
    "    for batch_idx, batch in enumerate(train_loader_phase2):\n",
    "        optimizer_phase2.zero_grad()\n",
    "        \n",
    "        # Move batch data to device\n",
    "        input_ids1 = batch['input_ids1'].to(device)\n",
    "        attention_mask1 = batch['attention_mask1'].to(device)\n",
    "        input_ids2 = batch['input_ids2'].to(device)\n",
    "        attention_mask2 = batch['attention_mask2'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        emb1 = model(input_ids1, attention_mask1)\n",
    "        emb2 = model(input_ids2, attention_mask2)\n",
    "        \n",
    "        # Ensure there are positive samples for InfoNCE calculation in the batch\n",
    "        if (labels == 1).sum() == 0:\n",
    "            print(f\"Phase 2 - Skipping batch {batch_idx+1} due to no positive samples.\")\n",
    "            continue # Skip batch if no positive pairs exist\n",
    "\n",
    "        loss = criterion(emb1, emb2, labels)\n",
    "        \n",
    "        # Check for NaN loss\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Phase 2 - NaN loss detected at Epoch {epoch+1}, Batch {batch_idx+1}. Skipping batch.\")\n",
    "            # Log problematic batch data\n",
    "            try:\n",
    "                wandb.log({\"phase2_problem_batch_data\": wandb.Table(data=[batch['cause_text'], batch['effect_text'], batch['label'].tolist()])})\n",
    "            except Exception as e:\n",
    "                print(f\"Could not log problematic batch to W&B: {e}\")\n",
    "            continue \n",
    "        \n",
    "        print(f\"Batch {batch_idx+1}\")\n",
    "    \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer_phase2.step()\n",
    "    \n",
    "    current_loss = loss.item()\n",
    "    total_loss_phase2 += current_loss\n",
    "    batch_losses_phase2.append(current_loss)\n",
    "    \n",
    "    # Log batch loss and learning rate to W&B (Phase 2)\n",
    "    current_lr_phase2 = optimizer_phase2.param_groups[0]['lr']\n",
    "    wandb.log({\n",
    "      \"phase2_batch_loss\": current_loss,\n",
    "      \"phase2_learning_rate\": current_lr_phase2,\n",
    "      \"phase2_epoch_frac\": epoch + (batch_idx + 1) / len(train_loader_phase2) # Log fractional epoch for phase 2\n",
    "    })\n",
    "    \n",
    "    if (batch_idx + 1) % 100 == 0: # Print progress every 100 batches\n",
    "        print(f\"Phase 2 - Epoch {epoch+1} Batch {batch_idx+1}/{len(train_loader_phase2)} - Loss {current_loss:.4f}, LR: {current_lr_phase2:.2e}\")\n",
    "\n",
    "    avg_loss_phase2 = total_loss_phase2 / len(batch_losses_phase2) if batch_losses_phase2 else 0 \n",
    "    train_losses_phase2.append(avg_loss_phase2) \n",
    "\n",
    "    # Evaluation step (Phase 2)\n",
    "    model.eval()\n",
    "    y_true_phase2, y_scores_phase2 = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader_phase2:\n",
    "            # Move batch data to device\n",
    "            input_ids1 = batch['input_ids1'].to(device)\n",
    "            attention_mask1 = batch['attention_mask1'].to(device)\n",
    "            input_ids2 = batch['input_ids2'].to(device)\n",
    "            attention_mask2 = batch['attention_mask2'].to(device)\n",
    "            \n",
    "            emb1 = model(input_ids1, attention_mask1)\n",
    "            emb2 = model(input_ids2, attention_mask2)\n",
    "            sim = F.cosine_similarity(emb1, emb2, dim=1).cpu().numpy()\n",
    "            y_scores_phase2.extend(sim.tolist())\n",
    "            y_true_phase2.extend(batch['label'].numpy().tolist())\n",
    "\n",
    "    # Ensure evaluation data is valid before calculating AUC\n",
    "    if len(np.unique(y_true_phase2)) > 1: \n",
    "        auc_phase2 = M.roc_auc_score(y_true_phase2, y_scores_phase2)\n",
    "    else:\n",
    "        print(f\"Phase 2 - Warning: Only one class present in evaluation labels for Epoch {epoch+1}. AUC cannot be calculated.\")\n",
    "        auc_phase2 = 0.0 \n",
    "\n",
    "    eval_aucs_phase2.append(auc_phase2) \n",
    "\n",
    "    # Log epoch metrics to W&B (Phase 2)\n",
    "    wandb.log({\n",
    "        \"phase2_epoch\": epoch + 1, \n",
    "        \"phase2_train_loss_epoch\": avg_loss_phase2,\n",
    "        \"phase2_eval_auc_epoch\": auc_phase2\n",
    "    })\n",
    "\n",
    "    print(f\"Phase 2 - Epoch {epoch+1}, Avg Train Loss: {avg_loss_phase2:.4f}, Eval AUC: {auc_phase2:.4f}\")\n",
    "  \n",
    "    # Early Stopping Check (Phase 2)\n",
    "    if auc_phase2 > best_eval_auc_phase2:\n",
    "        best_eval_auc_phase2 = auc_phase2\n",
    "        epochs_no_improve_phase2 = 0\n",
    "        # Save the best model checkpoint for phase 2\n",
    "        best_model_path_phase2 = os.path.join(wandb.run.dir, 'best_model_phase2.pth') \n",
    "        torch.save(model.state_dict(), best_model_path_phase2)\n",
    "        print(f\"Phase 2 - New best model saved with AUC: {best_eval_auc_phase2:.4f}\")\n",
    "        wandb.save(best_model_path_phase2) # Save best model to W&B\n",
    "    else:\n",
    "        epochs_no_improve_phase2 += 1\n",
    "        print(f\"Phase 2 - No improvement in Eval AUC for {epochs_no_improve_phase2} epoch(s).\")\n",
    "\n",
    "    if epochs_no_improve_phase2 >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"Phase 2 - Early stopping triggered after {epoch + 1} epochs.\")\n",
    "        break # Exit training loop\n",
    "\n",
    "    # Step the scheduler (Phase 2)\n",
    "    scheduler_phase2.step()\n",
    "\n",
    "print(\"Training Phase 2 finished.\")\n",
    "# Log final best AUC for Phase 2\n",
    "wandb.summary[\"phase2_best_eval_auc\"] = best_eval_auc_phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26098d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on Phase 2 eval set\n",
    "print(\"\\nFinal Evaluation on Phase 2 Eval Set...\")\n",
    "model.eval()\n",
    "y_true_final_p2, y_scores_final_p2 = [], []\n",
    "with torch.no_grad():\n",
    "  for batch in eval_loader_phase2:\n",
    "    # Move batch data to device\n",
    "    input_ids1 = batch['input_ids1'].to(device)\n",
    "    attention_mask1 = batch['attention_mask1'].to(device)\n",
    "    input_ids2 = batch['input_ids2'].to(device)\n",
    "    attention_mask2 = batch['attention_mask2'].to(device)\n",
    "      \n",
    "    emb1 = model(input_ids1, attention_mask1)\n",
    "    emb2 = model(input_ids2, attention_mask2)\n",
    "    sim = torch.nn.functional.cosine_similarity(emb1, emb2, dim=1).cpu().numpy()\n",
    "    y_scores_final_p2.extend(sim.tolist())\n",
    "    y_true_final_p2.extend(batch['label'].numpy().tolist())\n",
    "\n",
    "if len(np.unique(y_true_final_p2)) > 1:\n",
    "    final_auc_p2 = M.roc_auc_score(y_true_final_p2, y_scores_final_p2)\n",
    "    print(f\"Final Eval AUC (Phase 2): {final_auc_p2:.4f}\")\n",
    "    wandb.summary[\"final_eval_auc_phase2\"] = final_auc_p2\n",
    "else:\n",
    "    print(\"Final Eval AUC (Phase 2): Could not be calculated (only one class in eval set).\")\n",
    "    wandb.summary[\"final_eval_auc_phase2\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final embeddings after Phase 2\n",
    "print(\"\\nSaving final causal embeddings after Phase 2...\")\n",
    "final_embeds_path = os.path.join(os.getcwd(), '..', 'models', 'causal_embeds_final_atomic_cnc.pth')\n",
    "os.makedirs(os.path.dirname(final_embeds_path), exist_ok=True)\n",
    "embeds_final = model.encoder.wte.weight.data.cpu()\n",
    "torch.save(embeds_final, final_embeds_path)\n",
    "print(f\"Final causal embeddings saved to {final_embeds_path}\")\n",
    "wandb.save(final_embeds_path) # Save final embeddings to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfd5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualization of Phase 2 results\n",
    "print(\"\\nPlotting Phase 2 training curves...\")\n",
    "# Determine the actual number of epochs run in Phase 2\n",
    "actual_epochs_phase2 = len(train_losses_phase2) \n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "if actual_epochs_phase2 > 0:\n",
    "    plt.plot(range(1, actual_epochs_phase2 + 1), train_losses_phase2, '-o')\n",
    "plt.title('Phase 2: Average Train Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "if actual_epochs_phase2 > 0:\n",
    "    plt.plot(range(1, actual_epochs_phase2 + 1), eval_aucs_phase2, '-o')\n",
    "plt.title('Phase 2: Evaluation AUC per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log the final plot to W&B (Phase 2 run)\n",
    "try:\n",
    "    wandb.log({\"phase2_training_curves\": plt}) \n",
    "except Exception as e:\n",
    "    print(f\"Could not log plot to W&B: {e}\")\n",
    "\n",
    "# Finish the W&B run for Phase 1 here\n",
    "print(\"Finishing W&B run for Phase 1...\")\n",
    "try:\n",
    "    wandb.finish()\n",
    "    print(\"Phase 1 W&B run finished.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error finishing Phase 1 W&B run: {e}. Continuing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4bd1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(os.getcwd(), '..', 'models'), exist_ok=True)\n",
    "embeds = model.encoder.wte.weight.data.cpu()\n",
    "torch.save(embeds, os.path.join(os.getcwd(), '..', 'models', 'causal_embeds.pth'))\n",
    "print(\"Causal embeddings saved to ../models/causal_embeds.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70ca67",
   "metadata": {},
   "source": [
    "## How GPT-2 Embeddings Fine-Tuning Works\n",
    "\n",
    "In this notebook, GPT-2 embeddings are fine-tuned using contrastive learning:\n",
    "\n",
    "- For each pair (cause, effect, label), texts are tokenized using GPT2Tokenizer and passed through GPT2Model.\n",
    "- The resulting embeddings (averaged across tokens) are used as representations of the cause and effect.\n",
    "- The contrastive loss function (ContrastiveLoss) minimizes the distance between embeddings for positive pairs (real causal relationships) and maximizes it for negative (alternative) pairs.\n",
    "- Gradients propagate only through GPT2Model parameters (including the embedding layer), which leads to fine-tuning of embeddings for the causal distinction task.\n",
    "- After training, GPT-2 embeddings become more sensitive to causal relationships, which can be used for integration into downstream tasks or attention modification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032e1a4",
   "metadata": {},
   "source": [
    "# Next: Improving Convergence\n",
    "- Add a projection head (MLP) on top of embeddings: GPT2 embedding → Linear → ReLU → Linear, then L2-normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb0499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the actual number of epochs run\n",
    "actual_epochs = len(train_losses) # Use the length of the recorded losses/aucs\n",
    "\n",
    "# Plot training loss and evaluation AUC\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, actual_epochs + 1), train_losses, '-o')\n",
    "plt.title('Average Train Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, actual_epochs + 1), eval_aucs, '-o')\n",
    "plt.title('Evaluation AUC per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log the final plot to W&B\n",
    "wandb.log({\"training_curves\": plt}) # Log the matplotlib figure object\n",
    "\n",
    "# Finish the W&B run for Phase 1 here\n",
    "print(\"Finishing W&B run for Phase 1...\")\n",
    "try:\n",
    "    wandb.finish()\n",
    "    print(\"Phase 1 W&B run finished.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error finishing Phase 1 W&B run: {e}. Continuing...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
